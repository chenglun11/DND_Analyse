#!/usr/bin/env python3
"""
æ‰¹é‡è´¨é‡è¯„ä¼°è„šæœ¬
ä¸ºæ‰€æœ‰è½¬æ¢åçš„åœ°ç‰¢åœ°å›¾æ–‡ä»¶ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any

from .quality_assessor import DungeonQualityAssessor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def assess_all_maps(input_dir: str = "output", output_dir: str = "output/reports") -> Dict[str, Any]:
    """è¯„ä¼°ç›®å½•ä¸­æ‰€æœ‰ç»Ÿä¸€æ ¼å¼çš„åœ°ç‰¢åœ°å›¾æ–‡ä»¶"""
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»Ÿä¸€æ ¼å¼çš„JSONæ–‡ä»¶
    json_files = list(input_path.glob("*.json"))
    
    # è¿‡æ»¤æ‰æŠ¥å‘Šæ–‡ä»¶
    json_files = [f for f in json_files if not f.name.startswith("quality_report") and not f.name.startswith("report")]
    
    logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªåœ°å›¾æ–‡ä»¶è¿›è¡Œè¯„ä¼°")
    
    results = {}
    assessor = DungeonQualityAssessor()
    
    for json_file in json_files:
        try:
            logger.info(f"è¯„ä¼°æ–‡ä»¶: {json_file.name}")
            
            # è¯»å–åœ°å›¾æ•°æ®
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è¯„ä¼°è´¨é‡
            metrics = assessor.assess_quality(data)
            
            # ä¿å­˜å•ç‹¬çš„æŠ¥å‘Š
            report_file = output_path / f"quality_report_{json_file.stem}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, ensure_ascii=False, indent=2)
            
            # æ”¶é›†ç»“æœ
            results[json_file.name] = {
                'overall_score': metrics['overall_score'],
                'grade': metrics['grade'],
                'detailed_metrics': metrics['scores'],
                'recommendations': metrics['recommendations']
            }
            
            logger.info(f"âœ“ {json_file.name}: {metrics['overall_score']:.3f} ({metrics['grade']})")
            
        except Exception as e:
            logger.error(f"è¯„ä¼° {json_file.name} æ—¶å‡ºé”™: {e}")
            results[json_file.name] = {
                'error': str(e),
                'overall_score': 0.0,
                'grade': 'é”™è¯¯'
            }
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary_report = generate_summary_report(results)
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_file = output_path / "quality_summary_report.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ±‡æ€»æŠ¥å‘Š
    print_summary_report(summary_report)
    
    return results

def generate_summary_report(results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    
    valid_results = {k: v for k, v in results.items() if 'error' not in v}
    
    if not valid_results:
        return {
            'summary': 'æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ',
            'total_files': len(results),
            'valid_files': 0,
            'error_files': len(results)
        }
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    scores = [r['overall_score'] for r in valid_results.values()]
    avg_score = sum(scores) / len(scores)
    max_score = max(scores)
    min_score = min(scores)
    
    # æŒ‰ç­‰çº§åˆ†ç±»
    grade_counts = {}
    for result in valid_results.values():
        grade = result['grade']
        grade_counts[grade] = grade_counts.get(grade, 0) + 1
    
    # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®çš„åœ°å›¾
    best_map = max(valid_results.items(), key=lambda x: x[1]['overall_score'])
    worst_map = min(valid_results.items(), key=lambda x: x[1]['overall_score'])
    
    # è®¡ç®—å„æŒ‡æ ‡çš„ç»Ÿè®¡
    metric_stats = {}
    metrics = ['accessibility', 'degree_variance', 'path_diversity', 
               'loop_ratio', 'door_distribution']
    
    for metric in metrics:
        values = []
        for r in valid_results.values():
            metric_result = r['detailed_metrics'].get(metric, {})
            if isinstance(metric_result, dict):
                score = metric_result.get('score', 0.0)
            else:
                score = metric_result
            values.append(score)
        metric_stats[metric] = {
            'average': sum(values) / len(values),
            'max': max(values),
            'min': min(values)
        }
    
    return {
        'summary': {
            'total_files': len(results),
            'valid_files': len(valid_results),
            'error_files': len(results) - len(valid_results),
            'average_score': avg_score,
            'max_score': max_score,
            'min_score': min_score,
            'best_map': {
                'name': best_map[0],
                'score': best_map[1]['overall_score'],
                'grade': best_map[1]['grade']
            },
            'worst_map': {
                'name': worst_map[0],
                'score': worst_map[1]['overall_score'],
                'grade': worst_map[1]['grade']
            }
        },
        'grade_distribution': grade_counts,
        'metric_statistics': metric_stats,
        'detailed_results': valid_results
    }

def print_summary_report(report: Dict[str, Any]) -> None:
    """æ‰“å°æ±‡æ€»æŠ¥å‘Šåˆ°æ§åˆ¶å°"""
    
    print("\n" + "="*60)
    print("åœ°ç‰¢åœ°å›¾è´¨é‡è¯„ä¼°æ±‡æ€»æŠ¥å‘Š")
    print("="*60)
    
    summary = report['summary']
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"  æœ‰æ•ˆæ–‡ä»¶: {summary['valid_files']}")
    print(f"  é”™è¯¯æ–‡ä»¶: {summary['error_files']}")
    print(f"  å¹³å‡è¯„åˆ†: {summary['average_score']:.3f}")
    print(f"  æœ€é«˜è¯„åˆ†: {summary['max_score']:.3f}")
    print(f"  æœ€ä½è¯„åˆ†: {summary['min_score']:.3f}")
    
    print(f"\nğŸ† æœ€ä½³åœ°å›¾:")
    best = summary['best_map']
    print(f"  {best['name']}: {best['score']:.3f} ({best['grade']})")
    
    print(f"\nâš ï¸  æœ€å·®åœ°å›¾:")
    worst = summary['worst_map']
    print(f"  {worst['name']}: {worst['score']:.3f} ({worst['grade']})")
    
    print(f"\nğŸ“ˆ ç­‰çº§åˆ†å¸ƒ:")
    for grade, count in report['grade_distribution'].items():
        print(f"  {grade}: {count} ä¸ª")
    
    print(f"\nğŸ“‹ æŒ‡æ ‡ç»Ÿè®¡:")
    for metric, stats in report['metric_statistics'].items():
        metric_name = {
            'accessibility': 'å¯è¾¾æ€§',
            'degree_variance': 'åº¦å·®',
            'path_diversity': 'è·¯å¾„å¤šæ ·æ€§',
            'loop_ratio': 'å›ç¯ç‡',
            'door_distribution': 'é—¨åˆ†å¸ƒ'
        }.get(metric, metric)
        print(f"  {metric_name}: å¹³å‡ {stats['average']:.3f}, æœ€é«˜ {stats['max']:.3f}, æœ€ä½ {stats['min']:.3f}")
    
    print("="*60)

def batch_assess_quality(input_dir: str, output_file: str, enable_spatial_inference: bool = True, adjacency_threshold: float = 1.0):
    """æ‰¹é‡è¯„ä¼°åœ°å›¾è´¨é‡ - CLI è°ƒç”¨çš„æ¥å£å‡½æ•°"""
    try:
        results = assess_all_maps(input_dir, os.path.dirname(output_file))
        
        # ä¿å­˜åˆ°æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰¹é‡è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return results
        
    except Exception as e:
        logger.error(f"æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡è¯„ä¼°åœ°ç‰¢åœ°å›¾è´¨é‡')
    parser.add_argument('--input', default='output', help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--output', default='output/reports', help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    logger.info("å¼€å§‹æ‰¹é‡è´¨é‡è¯„ä¼°...")
    results = assess_all_maps(args.input, args.output)
    logger.info("æ‰¹é‡è´¨é‡è¯„ä¼°å®Œæˆ!")

if __name__ == '__main__':
    main() 