#!/usr/bin/env python3
"""
æ‰¹é‡è´¨é‡è¯„ä¼°è„šæœ¬
ä¸ºæ‰€æœ‰è½¬æ¢åçš„åœ°ç‰¢åœ°å›¾æ–‡ä»¶ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š
"""

import os
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any
import signal

from .quality_assessor import DungeonQualityAssessor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TimeoutError(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutError("æ“ä½œè¶…æ—¶")

def assess_all_maps(input_dir: str = "output", output_dir: str = "output/reports", timeout_per_file: int = 30) -> Dict[str, Any]:
    """è¯„ä¼°ç›®å½•ä¸­æ‰€æœ‰ç»Ÿä¸€æ ¼å¼çš„åœ°ç‰¢åœ°å›¾æ–‡ä»¶"""
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»Ÿä¸€æ ¼å¼çš„JSONæ–‡ä»¶
    json_files = list(input_path.glob("*.json"))
    
    # è¿‡æ»¤æ‰æŠ¥å‘Šæ–‡ä»¶
    json_files = [f for f in json_files if not f.name.startswith("quality_report") and not f.name.startswith("report")]
    
    logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªåœ°å›¾æ–‡ä»¶è¿›è¡Œè¯„ä¼°")
    
    results = {}
    assessor = DungeonQualityAssessor()
    
    for i, json_file in enumerate(json_files, 1):
        try:
            logger.info(f"è¯„ä¼°æ–‡ä»¶ [{i}/{len(json_files)}]: {json_file.name}")
            
            # è®¾ç½®è¶…æ—¶
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_per_file)
            
            try:
                # è¯»å–åœ°å›¾æ•°æ®
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è¯„ä¼°è´¨é‡
                start_time = time.time()
                metrics = assessor.assess_quality(data)
                end_time = time.time()
                
                # å–æ¶ˆè¶…æ—¶
                signal.alarm(0)
                
                # ä¿å­˜å•ç‹¬çš„æŠ¥å‘Š
                report_file = output_path / f"quality_report_{json_file.stem}.json"
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(metrics, f, ensure_ascii=False, indent=2)
                
                # æ”¶é›†ç»“æœ
                results[json_file.name] = {
                    'overall_score': metrics['overall_score'],
                    'grade': metrics['grade'],
                    'detailed_metrics': metrics['scores'],
                    'recommendations': metrics['recommendations'],
                    'processing_time': end_time - start_time
                }
                
                logger.info(f"âœ“ {json_file.name}: {metrics['overall_score']:.3f} ({metrics['grade']}) - {end_time - start_time:.2f}s")
                
            except TimeoutError:
                signal.alarm(0)
                logger.error(f"è¯„ä¼° {json_file.name} è¶…æ—¶")
                results[json_file.name] = {
                    'error': 'è¯„ä¼°è¶…æ—¶',
                    'overall_score': 0.0,
                    'grade': 'è¶…æ—¶'
                }
            except Exception as e:
                signal.alarm(0)
                logger.error(f"è¯„ä¼° {json_file.name} æ—¶å‡ºé”™: {e}")
                results[json_file.name] = {
                    'error': str(e),
                    'overall_score': 0.0,
                    'grade': 'é”™è¯¯'
                }
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {json_file.name} æ—¶å‡ºç°æ„å¤–é”™è¯¯: {e}")
            results[json_file.name] = {
                'error': f'æ„å¤–é”™è¯¯: {str(e)}',
                'overall_score': 0.0,
                'grade': 'é”™è¯¯'
            }
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    try:
        summary_report = generate_summary_report(results)
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_file = output_path / "quality_summary_report.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ±‡æ€»æŠ¥å‘Š
        print_summary_report(summary_report)
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        # å³ä½¿æ±‡æ€»æŠ¥å‘Šå¤±è´¥ï¼Œä¹Ÿè¿”å›å·²å¤„ç†çš„ç»“æœ
        results['_summary_error'] = str(e)
    
    return results

def generate_summary_report(results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    
    # è¿‡æ»¤æ‰é”™è¯¯æ ‡è®°
    valid_results = {k: v for k, v in results.items() if 'error' not in v and not k.startswith('_')}
    
    if not valid_results:
        return {
            'summary': 'æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ',
            'total_files': len(results),
            'valid_files': 0,
            'error_files': len(results)
        }
    
    try:
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        scores = [r['overall_score'] for r in valid_results.values()]
        avg_score = sum(scores) / len(scores)
        max_score = max(scores)
        min_score = min(scores)
        
        # æŒ‰ç­‰çº§åˆ†ç±»
        grade_counts = {}
        for result in valid_results.values():
            grade = result['grade']
            grade_counts[grade] = grade_counts.get(grade, 0) + 1
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®çš„åœ°å›¾
        best_map = max(valid_results.items(), key=lambda x: x[1]['overall_score'])
        worst_map = min(valid_results.items(), key=lambda x: x[1]['overall_score'])
        
        # è®¡ç®—å„æŒ‡æ ‡çš„ç»Ÿè®¡
        metric_stats = {}
        metrics = ['accessibility', 'degree_variance', 'path_diversity', 
                   'loop_ratio', 'door_distribution', 'treasure_monster_distribution', 'aesthetic_balance']
        
        for metric in metrics:
            values = []
            for r in valid_results.values():
                metric_result = r['detailed_metrics'].get(metric, {})
                if isinstance(metric_result, dict):
                    score = metric_result.get('score', 0.0)
                else:
                    score = metric_result
                values.append(score)
            
            if values:  # ç¡®ä¿æœ‰æœ‰æ•ˆå€¼
                metric_stats[metric] = {
                    'average': sum(values) / len(values),
                    'max': max(values),
                    'min': min(values)
                }
        
        return {
            'summary': {
                'total_files': len(results),
                'valid_files': len(valid_results),
                'error_files': len(results) - len(valid_results),
                'average_score': avg_score,
                'max_score': max_score,
                'min_score': min_score,
                'best_map': {
                    'name': best_map[0],
                    'score': best_map[1]['overall_score'],
                    'grade': best_map[1]['grade']
                },
                'worst_map': {
                    'name': worst_map[0],
                    'score': worst_map[1]['overall_score'],
                    'grade': worst_map[1]['grade']
                }
            },
            'grade_distribution': grade_counts,
            'metric_statistics': metric_stats,
            'detailed_results': valid_results
        }
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        return {
            'summary': f'æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}',
            'total_files': len(results),
            'valid_files': len(valid_results),
            'error_files': len(results) - len(valid_results)
        }

def print_summary_report(report: Dict[str, Any]) -> None:
    """æ‰“å°æ±‡æ€»æŠ¥å‘Šåˆ°æ§åˆ¶å°"""
    
    print("\n" + "="*60)
    print("åœ°ç‰¢åœ°å›¾è´¨é‡è¯„ä¼°æ±‡æ€»æŠ¥å‘Š")
    print("="*60)
    
    if 'summary' not in report or isinstance(report['summary'], str):
        print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report.get('summary', 'æœªçŸ¥é”™è¯¯')}")
        print("="*60)
        return
    
    summary = report['summary']
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"  æœ‰æ•ˆæ–‡ä»¶: {summary['valid_files']}")
    print(f"  é”™è¯¯æ–‡ä»¶: {summary['error_files']}")
    print(f"  å¹³å‡è¯„åˆ†: {summary['average_score']:.3f}")
    print(f"  æœ€é«˜è¯„åˆ†: {summary['max_score']:.3f}")
    print(f"  æœ€ä½è¯„åˆ†: {summary['min_score']:.3f}")
    
    if 'best_map' in summary:
        print(f"\nğŸ† æœ€ä½³åœ°å›¾:")
        best = summary['best_map']
        print(f"  {best['name']}: {best['score']:.3f} ({best['grade']})")
        
        print(f"\nâš ï¸  æœ€å·®åœ°å›¾:")
        worst = summary['worst_map']
        print(f"  {worst['name']}: {worst['score']:.3f} ({worst['grade']})")
    
    if 'grade_distribution' in report:
        print(f"\nğŸ“ˆ ç­‰çº§åˆ†å¸ƒ:")
        for grade, count in report['grade_distribution'].items():
            print(f"  {grade}: {count} ä¸ª")
    
    if 'metric_statistics' in report:
        print(f"\nğŸ“‹ æŒ‡æ ‡ç»Ÿè®¡:")
        for metric, stats in report['metric_statistics'].items():
            metric_name = {
                'accessibility': 'å¯è¾¾æ€§',
                'degree_variance': 'åº¦å·®',
                'path_diversity': 'è·¯å¾„å¤šæ ·æ€§',
                'loop_ratio': 'å›ç¯ç‡',
                'door_distribution': 'é—¨åˆ†å¸ƒ',
                'treasure_monster_distribution': 'å®è—æ€ªç‰©åˆ†å¸ƒ',
                'aesthetic_balance': 'è§†è§‰å¹³è¡¡'
            }.get(metric, metric)
            print(f"  {metric_name}: å¹³å‡ {stats['average']:.3f}, æœ€é«˜ {stats['max']:.3f}, æœ€ä½ {stats['min']:.3f}")
    
    print("="*60)

def batch_assess_quality(input_dir: str, output_dir: str, enable_spatial_inference: bool = True, adjacency_threshold: float = 1.0, timeout_per_file: int = 30):
    """æ‰¹é‡è¯„ä¼°åœ°å›¾è´¨é‡ - CLI è°ƒç”¨çš„æ¥å£å‡½æ•°"""
    try:
        logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼°ï¼Œè¾“å…¥ç›®å½•: {input_dir}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"æ¯ä¸ªæ–‡ä»¶è¶…æ—¶æ—¶é—´: {timeout_per_file}ç§’")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        input_dir_name = os.path.basename(input_dir.rstrip('/'))
        output_file = os.path.join(output_dir, f"{input_dir_name}_batch_report.json")
        
        results = assess_all_maps(input_dir, output_dir, timeout_per_file)
        
        # ä¿å­˜åˆ°æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰¹é‡è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return results
        
    except Exception as e:
        logger.error(f"æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡è¯„ä¼°åœ°ç‰¢åœ°å›¾è´¨é‡')
    parser.add_argument('--input', default='output', help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--output', default='output/reports', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--timeout', type=int, default=30, help='æ¯ä¸ªæ–‡ä»¶çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰')
    
    args = parser.parse_args()
    
    logger.info("å¼€å§‹æ‰¹é‡è´¨é‡è¯„ä¼°...")
    results = assess_all_maps(args.input, args.output, args.timeout)
    logger.info("æ‰¹é‡è´¨é‡è¯„ä¼°å®Œæˆ!")

if __name__ == '__main__':
    main() 