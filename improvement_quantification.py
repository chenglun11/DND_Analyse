#!/usr/bin/env python3
"""
æ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°ç³»ç»Ÿ
æä¾›å¤šç§é‡åŒ–æŒ‡æ ‡æ¥è¯„ä¼°åœ°ç‰¢ç”Ÿæˆæ–¹æ³•çš„æ”¹è¿›æ•ˆæœ
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from scipy import stats
from dataclasses import dataclass

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('src')
from quality_assessor import DungeonQualityAssessor
from benchmark_standards import DungeonBenchmarkStandards

@dataclass
class ImprovementMetrics:
    """æ”¹è¿›æ•ˆæœæŒ‡æ ‡"""
    metric_name: str
    before_mean: float
    after_mean: float
    improvement_pct: float
    effect_size: float  # Cohen's d
    t_statistic: float
    p_value: float
    significance: bool
    benchmark_grade_before: str
    benchmark_grade_after: str
    grade_improvement: bool

class ImprovementQuantifier:
    """æ”¹è¿›æ•ˆæœé‡åŒ–å™¨"""
    
    def __init__(self):
        self.assessor = DungeonQualityAssessor()
        self.standards = DungeonBenchmarkStandards()
        
    def quantify_improvement(self, before_dir: str, after_dir: str) -> Dict:
        """é‡åŒ–æ”¹è¿›æ•ˆæœ"""
        print("ğŸ” å¼€å§‹é‡åŒ–æ”¹è¿›æ•ˆæœ...")
        
        # æ”¶é›†æ•°æ®
        before_data = self._collect_data(before_dir, "æ”¹è¿›å‰")
        after_data = self._collect_data(after_dir, "æ”¹è¿›å")
        
        if before_data.empty or after_data.empty:
            return {"error": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé‡åŒ–åˆ†æ"}
        
        # è®¡ç®—å„é¡¹æ”¹è¿›æŒ‡æ ‡
        improvement_metrics = self._calculate_improvement_metrics(before_data, after_data)
        
        # è®¡ç®—ç»¼åˆæ”¹è¿›åˆ†æ•°
        overall_improvement = self._calculate_overall_improvement(improvement_metrics)
        
        # ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š
        improvement_report = self._generate_improvement_report(
            improvement_metrics, overall_improvement, before_data, after_data
        )
        
        return improvement_report
    
    def _collect_data(self, data_dir: str, label: str) -> pd.DataFrame:
        """æ”¶é›†è¯„ä¼°æ•°æ®"""
        records = []
        
        if not os.path.exists(data_dir):
            print(f"è­¦å‘Š: ç›®å½• {data_dir} ä¸å­˜åœ¨")
            return pd.DataFrame()
        
        json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]
        
        for fname in json_files:
            try:
                path = os.path.join(data_dir, fname)
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                result = self.assessor.assess_quality(data)
                
                row = {
                    'file': fname,
                    'label': label,
                    'overall_score': result['overall_score'],
                    'grade': result['grade']
                }
                
                # æ·»åŠ å„é¡¹æŒ‡æ ‡åˆ†æ•°
                for rule, score in result['scores'].items():
                    row[rule] = score
                
                records.append(row)
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {fname} æ—¶å‡ºé”™: {e}")
        
        return pd.DataFrame(records)
    
    def _calculate_improvement_metrics(self, before_data: pd.DataFrame, 
                                     after_data: pd.DataFrame) -> List[ImprovementMetrics]:
        """è®¡ç®—æ”¹è¿›æŒ‡æ ‡"""
        metrics = ['accessibility', 'dead_end_ratio', 'degree_variance', 
                  'door_distribution', 'key_path_length', 'loop_ratio', 'path_diversity']
        
        improvement_metrics = []
        
        for metric in metrics:
            before_scores = before_data[metric].dropna()
            after_scores = after_data[metric].dropna()
            
            if len(before_scores) > 0 and len(after_scores) > 0:
                # åŸºæœ¬ç»Ÿè®¡
                before_mean = before_scores.mean()
                after_mean = after_scores.mean()
                improvement_pct = ((after_mean - before_mean) / before_mean * 100) if before_mean > 0 else 0
                
                # æ•ˆåº”é‡ (Cohen's d)
                pooled_std = np.sqrt(((len(before_scores) - 1) * before_scores.var() + 
                                    (len(after_scores) - 1) * after_scores.var()) / 
                                   (len(before_scores) + len(after_scores) - 2))
                effect_size = (after_mean - before_mean) / pooled_std if pooled_std > 0 else 0
                
                # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
                if len(before_scores) > 1 and len(after_scores) > 1:
                    t_stat, p_value = stats.ttest_ind(before_scores, after_scores)
                    significance = p_value < 0.05
                else:
                    t_stat, p_value, significance = 0, 1, False
                
                # åŸºå‡†ç­‰çº§è¯„ä¼°
                benchmark_before = self.standards.evaluate_against_benchmark(before_mean, metric)
                benchmark_after = self.standards.evaluate_against_benchmark(after_mean, metric)
                grade_improvement = benchmark_after[1] > benchmark_before[1]
                
                improvement_metrics.append(ImprovementMetrics(
                    metric_name=metric,
                    before_mean=before_mean,
                    after_mean=after_mean,
                    improvement_pct=improvement_pct,
                    effect_size=effect_size,
                    t_statistic=t_stat,
                    p_value=p_value,
                    significance=significance,
                    benchmark_grade_before=benchmark_before[0],
                    benchmark_grade_after=benchmark_after[0],
                    grade_improvement=grade_improvement
                ))
        
        return improvement_metrics
    
    def _calculate_overall_improvement(self, improvement_metrics: List[ImprovementMetrics]) -> Dict:
        """è®¡ç®—ç»¼åˆæ”¹è¿›åˆ†æ•°"""
        if not improvement_metrics:
            return {"overall_score": 0.0, "grade": "no_improvement"}
        
        # è®¡ç®—åŠ æƒæ”¹è¿›åˆ†æ•°
        total_weighted_score = 0.0
        total_weight = 0.0
        significant_improvements = 0
        grade_improvements = 0
        
        # æŒ‡æ ‡æƒé‡ (åŸºäºé‡è¦æ€§)
        metric_weights = {
            'accessibility': 0.25,
            'path_diversity': 0.20,
            'dead_end_ratio': 0.15,
            'loop_ratio': 0.15,
            'degree_variance': 0.10,
            'door_distribution': 0.10,
            'key_path_length': 0.05
        }
        
        for metric in improvement_metrics:
            weight = metric_weights.get(metric.metric_name, 0.1)
            
            # æ”¹è¿›åˆ†æ•° (åŸºäºæ•ˆåº”é‡å’Œæ˜¾è‘—æ€§)
            improvement_score = 0.0
            if metric.significance:
                significant_improvements += 1
                if abs(metric.effect_size) >= 0.8:
                    improvement_score = 1.0  # å¤§æ•ˆåº”
                elif abs(metric.effect_size) >= 0.5:
                    improvement_score = 0.8  # ä¸­ç­‰æ•ˆåº”
                elif abs(metric.effect_size) >= 0.2:
                    improvement_score = 0.6  # å°æ•ˆåº”
                else:
                    improvement_score = 0.4  # å¾®å°æ•ˆåº”
            else:
                improvement_score = 0.2  # æ— æ˜¾è‘—æ”¹è¿›
            
            # å¦‚æœæ”¹è¿›ç™¾åˆ†æ¯”ä¸ºæ­£ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
            if metric.improvement_pct > 0:
                improvement_score *= 1.2
            
            if metric.grade_improvement:
                grade_improvements += 1
                improvement_score *= 1.1
            
            total_weighted_score += improvement_score * weight
            total_weight += weight
        
        overall_score = total_weighted_score / total_weight if total_weight > 0 else 0.0
        
        # ç¡®å®šæ€»ä½“æ”¹è¿›ç­‰çº§
        if overall_score >= 0.8:
            grade = "excellent_improvement"
        elif overall_score >= 0.6:
            grade = "good_improvement"
        elif overall_score >= 0.4:
            grade = "moderate_improvement"
        elif overall_score >= 0.2:
            grade = "minimal_improvement"
        else:
            grade = "no_improvement"
        
        return {
            "overall_score": overall_score,
            "grade": grade,
            "significant_improvements": significant_improvements,
            "total_metrics": len(improvement_metrics),
            "grade_improvements": grade_improvements,
            "improvement_rate": significant_improvements / len(improvement_metrics) if improvement_metrics else 0
        }
    
    def _generate_improvement_report(self, improvement_metrics: List[ImprovementMetrics],
                                   overall_improvement: Dict,
                                   before_data: pd.DataFrame,
                                   after_data: pd.DataFrame) -> Dict:
        """ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š"""
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        before_overall_mean = before_data['overall_score'].mean()
        after_overall_mean = after_data['overall_score'].mean()
        overall_improvement_pct = ((after_overall_mean - before_overall_mean) / before_overall_mean * 100) if before_overall_mean > 0 else 0
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        detailed_metrics = []
        for metric in improvement_metrics:
            detailed_metrics.append({
                "metric": metric.metric_name,
                "before_mean": round(metric.before_mean, 3),
                "after_mean": round(metric.after_mean, 3),
                "improvement_pct": round(metric.improvement_pct, 1),
                "effect_size": round(metric.effect_size, 3),
                "p_value": round(metric.p_value, 3),
                "significant": metric.significance,
                "benchmark_before": metric.benchmark_grade_before,
                "benchmark_after": metric.benchmark_grade_after,
                "grade_improvement": metric.grade_improvement
            })
        
        # æ”¹è¿›æ•ˆæœåˆ†ç±»
        improvements_by_category = {
            "major_improvements": [m for m in improvement_metrics if m.improvement_pct > 20 and m.significance],
            "moderate_improvements": [m for m in improvement_metrics if 5 <= m.improvement_pct <= 20 and m.significance],
            "minor_improvements": [m for m in improvement_metrics if 0 < m.improvement_pct < 5 and m.significance],
            "no_change": [m for m in improvement_metrics if abs(m.improvement_pct) < 1],
            "regressions": [m for m in improvement_metrics if m.improvement_pct < -1]
        }
        
        return {
            "summary": {
                "overall_improvement_score": round(overall_improvement["overall_score"], 3),
                "overall_grade": overall_improvement["grade"],
                "overall_improvement_pct": round(overall_improvement_pct, 1),
                "significant_improvements": overall_improvement["significant_improvements"],
                "total_metrics": overall_improvement["total_metrics"],
                "improvement_rate": round(overall_improvement["improvement_rate"] * 100, 1),
                "grade_improvements": overall_improvement["grade_improvements"]
            },
            "detailed_metrics": detailed_metrics,
            "improvements_by_category": {
                category: len(metrics) for category, metrics in improvements_by_category.items()
            },
            "sample_sizes": {
                "before": len(before_data),
                "after": len(after_data)
            }
        }
    
    def print_improvement_report(self, report: Dict):
        """æ‰“å°æ”¹è¿›æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ† æ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°æŠ¥å‘Š")
        print("="*80)
        
        summary = report["summary"]
        print(f"\nğŸ“Š æ€»ä½“æ”¹è¿›æ•ˆæœ:")
        print(f"  ç»¼åˆæ”¹è¿›åˆ†æ•°: {summary['overall_improvement_score']:.3f}")
        print(f"  æ”¹è¿›ç­‰çº§: {summary['overall_grade']}")
        print(f"  æ€»ä½“æ”¹è¿›ç™¾åˆ†æ¯”: {summary['overall_improvement_pct']:+.1f}%")
        print(f"  æ˜¾è‘—æ”¹è¿›æŒ‡æ ‡: {summary['significant_improvements']}/{summary['total_metrics']}")
        print(f"  æ”¹è¿›æˆåŠŸç‡: {summary['improvement_rate']}%")
        print(f"  ç­‰çº§æå‡æŒ‡æ ‡: {summary['grade_improvements']}")
        
        print(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡æ”¹è¿›:")
        for metric in report["detailed_metrics"]:
            direction = "â†—ï¸" if metric["improvement_pct"] > 0 else "â†˜ï¸"
            significance = "âœ…" if metric["significant"] else "âŒ"
            grade_improvement = "â­" if metric["grade_improvement"] else ""
            
            print(f"  {metric['metric']}: {metric['before_mean']:.3f} â†’ {metric['after_mean']:.3f} "
                  f"({metric['improvement_pct']:+.1f}%) {direction} {significance} {grade_improvement}")
        
        print(f"\nğŸ“‹ æ”¹è¿›åˆ†ç±»ç»Ÿè®¡:")
        for category, count in report["improvements_by_category"].items():
            if count > 0:
                print(f"  {category}: {count} ä¸ªæŒ‡æ ‡")
        
        print(f"\nğŸ“Š æ ·æœ¬ä¿¡æ¯:")
        print(f"  æ”¹è¿›å‰æ ·æœ¬: {report['sample_sizes']['before']} ä¸ªåœ°ç‰¢")
        print(f"  æ”¹è¿›åæ ·æœ¬: {report['sample_sizes']['after']} ä¸ªåœ°ç‰¢")
        
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°')
    parser.add_argument('--before', required=True, help='æ”¹è¿›å‰ç›®å½•')
    parser.add_argument('--after', required=True, help='æ”¹è¿›åç›®å½•')
    parser.add_argument('--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    
    args = parser.parse_args()
    
    quantifier = ImprovementQuantifier()
    report = quantifier.quantify_improvement(args.before, args.after)
    
    if "error" in report:
        print(f"é”™è¯¯: {report['error']}")
        return
    
    # æ‰“å°æŠ¥å‘Š
    quantifier.print_improvement_report(report)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")

if __name__ == '__main__':
    main() 